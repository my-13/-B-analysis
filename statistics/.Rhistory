Anova(m0, type=3)
#df$nTemplates = as.numeric(df$nTemplates) # templates as continuous
m1 = glmer(test_mean_1 ~ algoType * nTemplates * ptype + (1|pID), data=df, family=Gamma(link="inverse"))
summary(m1)
Anova(m1, type=3) ### preferred model
with(df, interaction.plot(
algoType, nTemplates, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
ylim=c(0,0.5),
main="Recognition Error Rate by Algorithm, No. Templates",
lwd=3,
lty=1,
col=rainbow(9)
))
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m1, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ nTemplates*algoType)), test=adjusted(type="holm"))
# no interactions
m0 = glmer(test_mean_1 ~ algoType + ptype + nTemplates + (1|pID), data=df, family=Gamma(link="inverse"))
summary(m0)
Anova(m0, type=3)
#df$nTemplates = as.numeric(df$nTemplates) # templates as continuous
m1 = glmer(test_mean_1 ~ algoType * nTemplates * ptype + (1|pID), data=df, family=Gamma(link="inverse"))
summary(m1)
Anova(m1, type=3) ### preferred model
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m1, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ nTemplates*algoType)), test=adjusted(type="holm"))
##
## 2. gesture_articulation.csv
##
df <- read.csv("gesture_articulation.csv")
df$pID = as.factor(df$pID)
df$algoType = as.factor(df$algoType)
df$gestureType = as.factor(df$gestureType)
df$ptype = as.factor(df$ptype)
contrasts(df$algoType) <- "contr.sum"
contrasts(df$gestureType) <- "contr.sum"
contrasts(df$ptype) <- "contr.sum"
View(df)
# EDA
ddply(df, ~ algoType + gestureType + ptype, function(data) c(
"Nrows"=nrow(data),
"Min"=min(data$test_mean),
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean),
"Var"=var(data$test_mean),
"Median"=median(data$test_mean),
"IQR"=IQR(data$test_mean),
"Max"=max(data$test_mean)
))
## algorithm
boxplot(test_mean ~ algoType,
xlab="Algorithm Type",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm",
col=c("lightblue","lightgreen","lightyellow"),
data=df)
m <- ddply(df, ~ algoType, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
b <- barplot(Mean ~ algoType,  # b stores X midpoint of each bar
xlab="Algorithm Type",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm",
col=c("lightblue","lightgreen","lightyellow"),
data=m)
arrows(x0 = b,  # error bars
y0 = m$Mean + m$SD,
y1 = m$Mean - m$SD,
angle=90,
code=3,
lwd=2,
length=0.3,
col=c("blue","darkgreen","goldenrod"))
## gesture type
boxplot(test_mean ~ gestureType,
xlab="Gesture Type",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Gesture Type",
col=c("lightblue","lightgreen","lightyellow"),
data=df)
m <- ddply(df, ~ gestureType, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
b <- barplot(Mean ~ gestureType,  # b stores X midpoint of each bar
xlab="Gesture Type",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Gesture Type",
col=c("lightblue","lightgreen","lightyellow"),
data=m)
arrows(x0 = b,  # error bars
y0 = m$Mean + m$SD,
y1 = m$Mean - m$SD,
angle=90,
code=3,
lwd=2,
length=0.3,
col=c("blue","darkgreen","goldenrod"))
## disability
boxplot(test_mean ~ ptype,
xlab="Disability",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Disability",
col=c("lightblue","lightgreen"),
data=df)
m <- ddply(df, ~ ptype, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
b <- barplot(Mean ~ ptype,  # b stores X midpoint of each bar
xlab="Disability",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Disability",
col=c("lightblue","lightgreen"),
data=m)
arrows(x0 = b,  # error bars
y0 = m$Mean + m$SD,
y1 = m$Mean - m$SD,
angle=90,
code=3,
lwd=2,
length=0.3,
col=c("blue","darkgreen"))
# Algorithm x Disability
with(df, interaction.plot(
algoType, ptype, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Disability",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, Disability",
lwd=3,
lty=1,
col=c("darkblue","darkgreen")
))
m <- ddply(df, ~ algoType + ptype, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
dx = 0.005  # nudge
arrows(x0=1-dx, y0=m[1,]$Mean - m[1,]$SD, x1=1-dx, y1=m[1,]$Mean + m[1,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=1+dx, y0=m[2,]$Mean - m[2,]$SD, x1=1+dx, y1=m[2,]$Mean + m[2,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=2-dx, y0=m[3,]$Mean - m[3,]$SD, x1=2-dx, y1=m[3,]$Mean + m[3,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=2+dx, y0=m[4,]$Mean - m[4,]$SD, x1=2+dx, y1=m[4,]$Mean + m[4,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=3-dx, y0=m[5,]$Mean - m[5,]$SD, x1=3-dx, y1=m[5,]$Mean + m[5,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=3+dx, y0=m[6,]$Mean - m[6,]$SD, x1=3+dx, y1=m[6,]$Mean + m[6,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
# Algorithm x Gesture Type
with(df, interaction.plot(
algoType, gestureType, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, Gesture Type",
lwd=3,
lty=1,
col=c("darkblue","darkgreen","goldenrod")
))
m <- ddply(df, ~ algoType + gestureType, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
dx = 0.015  # nudge
arrows(x0=1-dx, y0=m[1,]$Mean - m[1,]$SD, x1=1-dx, y1=m[1,]$Mean + m[1,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=1+00, y0=m[2,]$Mean - m[2,]$SD, x1=1+00, y1=m[2,]$Mean + m[2,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=1+dx, y0=m[3,]$Mean - m[3,]$SD, x1=1+dx, y1=m[3,]$Mean + m[3,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="goldenrod")
arrows(x0=2-dx, y0=m[4,]$Mean - m[4,]$SD, x1=2-dx, y1=m[4,]$Mean + m[4,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=2+00, y0=m[5,]$Mean - m[5,]$SD, x1=2+00, y1=m[5,]$Mean + m[5,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=2+dx, y0=m[6,]$Mean - m[6,]$SD, x1=2+dx, y1=m[6,]$Mean + m[6,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="goldenrod")
arrows(x0=3-dx, y0=m[7,]$Mean - m[7,]$SD, x1=3-dx, y1=m[7,]$Mean + m[7,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=3+00, y0=m[8,]$Mean - m[8,]$SD, x1=3+00, y1=m[8,]$Mean + m[8,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=3+dx, y0=m[9,]$Mean - m[9,]$SD, x1=3+dx, y1=m[9,]$Mean + m[9,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="goldenrod")
# Gesture Type x Disability
with(df, interaction.plot(
gestureType, ptype, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Disability",
ylim=c(0,1),
main="Recognition Error Rate by Gesture Type, Disability",
lwd=3,
lty=1,
col=c("darkblue","darkgreen")
))
m <- ddply(df, ~ gestureType + ptype, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
dx = 0.005  # nudge
arrows(x0=1-dx, y0=m[1,]$Mean - m[1,]$SD, x1=1-dx, y1=m[1,]$Mean + m[1,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=1+dx, y0=m[2,]$Mean - m[2,]$SD, x1=1+dx, y1=m[2,]$Mean + m[2,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=2-dx, y0=m[3,]$Mean - m[3,]$SD, x1=2-dx, y1=m[3,]$Mean + m[3,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=2+dx, y0=m[4,]$Mean - m[4,]$SD, x1=2+dx, y1=m[4,]$Mean + m[4,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=3-dx, y0=m[5,]$Mean - m[5,]$SD, x1=3-dx, y1=m[5,]$Mean + m[5,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=3+dx, y0=m[6,]$Mean - m[6,]$SD, x1=3+dx, y1=m[6,]$Mean + m[6,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
hist(df$test_mean, main="Histogram of Recognition Error Rate", freq=TRUE, xlab="Error Rate", xlim=c(0,1)) # frequency (counts)
hist(df$test_mean, main="Histogram of Recognition Error Rate", freq=FALSE, xlab="Error Rate", xlim=c(0,1)) # density (area sums to 1.00)
f = fitdistr(df$test_mean, "normal")$estimate  # normal
curve(dnorm(x, mean=f[1], sd=f[2]), col="blue", lty=1, lwd=3, add=TRUE) # add normal curve
ks.test(df$test_mean, "pnorm", mean=f[1], sd=f[2]) # p< 2.2e-16
f = fitdistr(df$test_mean, "lognormal")$estimate  # lognormal
curve(dlnorm(x, meanlog=f[1], sdlog=f[2]), col="purple", lty=1, lwd=3, add=TRUE) # add lognormal curve
ks.test(df$test_mean, "plnorm", meanlog=f[1], sdlog=f[2]) # p < 2.2e-16
f = fitdistr(round(df$test_mean,0), "Poisson")$estimate  # Poisson
curve(dpois(round(x,0), lambda=f[1]), col="gold", lty=1, lwd=3, add=TRUE) # add Poisson curve
ks.test(df$test_mean, "ppois", lambda=f[1]) # p < 2.2e-16
f = fitdistr(round(df$test_mean,0), "negative binomial", lower=1e-6)$estimate  # negative binomial
curve(dnbinom(round(x,0), size=f[1], mu=f[2]), col="darkgray", lty=1, lwd=3, add=TRUE) # add negative binomial curve
ks.test(df$test_mean, "pnbinom", size=f[1], mu=f[2]) # p < 2.2e-16
f = fitdistr(df$test_mean, "exponential")$estimate  # exponential
curve(dexp(x, rate=f[1]), col="darkred", lty=1, lwd=3, add=TRUE) # add exponential curve
ks.test(df$test_mean, "pexp", rate=f[1]) # p < 2.2e-16
df$test_mean_1 = df$test_mean + 1 ## add a new column
hist(df$test_mean_1, main="Histogram of Recognition Error Rate", freq=FALSE, xlab="Error Rate", xlim=c(1,2)) # density (area sums to 1.00)
f = fitdistr(df$test_mean_1, "gamma")$estimate  # Gamma
curve(dgamma(x, shape=f[1], rate=f[2]), col="darkgreen", lty=1, lwd=3, add=TRUE) # add Gamma curve
ks.test(df$test_mean_1, "pgamma", shape=f[1], rate=f[2]) # NaNs!
df$test_mean_100 = round(df$test_mean * 100, 0)
View(df)
hist(df$test_mean_100, main="Histogram of Recognition Errors per 100", freq=FALSE, xlab="Errors", xlim=c(0,100)) # density (area sums to 1.00)
f = fitdistr(round(df$test_mean_100,0), "Poisson")$estimate  # Poisson
curve(dpois(round(x,0), lambda=f[1]), col="gold", lty=1, lwd=3, add=TRUE) # add Poisson curve
ks.test(df$test_mean_100, "ppois", lambda=f[1]) # p < 2.2e-16
f = fitdistr(round(df$test_mean_100,0), "negative binomial", lower=1e-6)$estimate  # negative binomial
curve(dnbinom(round(x,0), size=f[1], mu=f[2]), col="darkgray", lty=1, lwd=3, add=TRUE) # add negative binomial curve
ks.test(df$test_mean_100, "pnbinom", size=f[1], mu=f[2]) # p < 2.2e-16
## ANOVA assumptions
m = aov_ez(dv="test_mean", within=c("algoType","gestureType","nTemplates"), between="ptype", id="pID", type=3, data=df)
r = residuals(m$lm)
length(r) # 2673
mean(r); sum(r) # should be ~0
plot(r[1:length(r)]); abline(h=0) # should be random
hist(r) # should be normal
hist(r, freq=FALSE)
f = fitdistr(r, "normal")$estimate  # normal
curve(dnorm(x, mean=f[1], sd=f[2]), col="blue", lty=1, lwd=3, add=TRUE) # add normal curve
ks.test(r, "pnorm", mean=f[1], sd=f[2]) # p = 6.993e-05
ks.test(r, "pnorm", mean=f[1], sd=f[2]) # p-value = 8.57e-08 # p = 6.993e-05
rg = as.numeric(unlist(r + abs(min(r)) + 1))
hist(rg, freq=FALSE)
f = fitdistr(rg, "gamma")$estimate  # Gamma
curve(dgamma(x, shape=f[1], rate=f[2]), col="darkgreen", lty=1, lwd=3, add=TRUE) # add Gamma curve
ks.test(rg, "pgamma", shape=f[1], rate=f[2]) # 0.0002077
ks.test(rg, "pgamma", shape=f[1], rate=f[2]) # p-value = 0.001616 # 0.0002077
qqnorm(r); qqline(r) # Q-Q plot
shapiro.test(r)  # p < 2.2e-16
shapiro.test(r)  # p-value = 1.469e-13 # p < 2.2e-16
ad.test(r)  # p < 2.2e-16
ad.test(r) # p-value < 2.2e-16 # p < 2.2e-16
print(check_homogeneity(m)) # Levene's Test: p = 0.896
print(check_normality(m))   # p < .001
## fit a linear mixed model (LMM)
m = lmer(test_mean ~ algoType * gestureType * ptype * nTemplates + (1|pID), data=df)
summary(m)
Anova(m, type=3)
#Poisson
m0 = glmer(test_mean_100 ~ algoType * gestureType * ptype * nTemplates + (1|pID), data=df, family=poisson)
summary(m0)
Anova(m0, type=3)
check_zeroinflation(m0)
m1 = glmmTMB(test_mean_100 ~ algoType * gestureType * ptype * nTemplates + (1|pID),
data=df,
family=poisson,
ziformula=~algoType*gestureType*ptype*nTemplates
)
## Gamma -- preferred
df$test_mean_1 = df$test_mean + 1
m = glmer(test_mean_1 ~ algoType * gestureType * ptype * nTemplates + (1|pID),
data=df,
family=Gamma(link="inverse")
)
summary(m)
Anova(m, type=3) ### preferred
## Gamma -- preferred
df$test_mean_1 = df$test_mean + 1
## Gamma -- preferred
df$test_mean_1 = df$test_mean + 1
m = glmer(test_mean_1 ~ algoType * gestureType * ptype * nTemplates + (1|pID),
data=df,
family=Gamma(link="inverse")
)
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
summary(glht(m, emm(pairwise ~ gestureType)), test=adjusted(type="holm"))
summary(glht(m, emm(pairwise ~ algoType*ptype)), test=adjusted(type="holm"))
summary(glht(m, emm(pairwise ~ algoType*gestureType*ptype)), test=adjusted(type="none"))
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
summary(glht(m, emm(pairwise ~ gestureType)), test=adjusted(type="holm"))
summary(glht(m, emm(pairwise ~ algoType*ptype)), test=adjusted(type="holm"))
##
## 3. user_independent.csv
##
df <- read.csv("user_independent.csv")
df$pID = as.factor(df$pID)
df$algoType = as.factor(df$algoType)
df$ptype = as.factor(df$ptype)
df$nTemplates = as.factor(df$nTemplates)
contrasts(df$algoType) <- "contr.sum"
contrasts(df$ptype) <- "contr.sum"
contrasts(df$nTemplates) <- "contr.sum"
View(df)
# EDA
ddply(df, ~ algoType + ptype, function(data) c(
"Nrows"=nrow(data),
"Min"=min(data$test_mean),
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean),
"Var"=var(data$test_mean),
"Median"=median(data$test_mean),
"IQR"=IQR(data$test_mean),
"Max"=max(data$test_mean)
))
boxplot(test_mean ~ algoType,
xlab="Algorithm Type",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm",
col=c("lightblue","lightgreen","lightyellow"),
data=df)
m <- ddply(df, ~ algoType, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
b <- barplot(Mean ~ algoType,  # b stores X midpoint of each bar
xlab="Algorithm Type",
ylab="Recognition Error Rate",
ylim=c(0, 0.8),
main="Recognition Error Rate by Algorithm",
col=c("lightblue","lightgreen","lightyellow"),
data=m)
arrows(x0 = b,  # error bars
y0 = m$Mean + m$SD,
y1 = m$Mean - m$SD,
angle=90,
code=3,
lwd=2,
length=0.3,
col=c("blue","darkgreen","goldenrod"))
boxplot(test_mean ~ ptype,
xlab="Disability",
ylab="Recognition Error Rate",
ylim=c(0,1),
main="Recognition Error Rate by Disability",
col=c("lightblue","lightgreen"),
data=df)
m <- ddply(df, ~ ptype, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
b <- barplot(Mean ~ ptype,  # b stores X midpoint of each bar
xlab="Disability",
ylab="Recognition Error Rate",
ylim=c(0, 0.8),
main="Recognition Error Rate by Disability",
col=c("lightblue","lightgreen"),
data=m)
arrows(x0 = b,  # error bars
y0 = m$Mean + m$SD,
y1 = m$Mean - m$SD,
angle=90,
code=3,
lwd=2,
length=0.3,
col=c("blue","darkgreen"))
with(df, interaction.plot(
algoType, ptype, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Disability",
ylim=c(0, 0.8),
main="Recognition Error Rate by Algorithm, Disability",
lwd=3,
lty=1,
col=c("darkblue","darkgreen")
))
m <- ddply(df, ~ algoType + ptype, function(data) c(
"Mean"=mean(data$test_mean),
"SD"=sd(data$test_mean)
))
dx = 0.005  # nudge
arrows(x0=1-dx, y0=m[1,]$Mean - m[1,]$SD, x1=1-dx, y1=m[1,]$Mean + m[1,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=1+dx, y0=m[2,]$Mean - m[2,]$SD, x1=1+dx, y1=m[2,]$Mean + m[2,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=2-dx, y0=m[3,]$Mean - m[3,]$SD, x1=2-dx, y1=m[3,]$Mean + m[3,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=2+dx, y0=m[4,]$Mean - m[4,]$SD, x1=2+dx, y1=m[4,]$Mean + m[4,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
arrows(x0=3-dx, y0=m[5,]$Mean - m[5,]$SD, x1=3-dx, y1=m[5,]$Mean + m[5,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkblue")
arrows(x0=3+dx, y0=m[6,]$Mean - m[6,]$SD, x1=3+dx, y1=m[6,]$Mean + m[6,]$SD, angle=90, code=3, lty=1, lwd=2, length=0.3, col="darkgreen")
hist(df$test_mean, main="Histogram of Recognition Error Rate", freq=TRUE, xlab="Error Rate", xlim=c(0,1)) # frequency (counts)
hist(df$test_mean, main="Histogram of Recognition Error Rate", freq=FALSE, xlab="Error Rate", xlim=c(0,1)) # density (area sums to 1.00)
f = fitdistr(df$test_mean, "normal")$estimate  # normal
curve(dnorm(x, mean=f[1], sd=f[2]), col="blue", lty=1, lwd=3, add=TRUE) # add normal curve
ks.test(df$test_mean, "pnorm", mean=f[1], sd=f[2]) # p = 0.1768
f = fitdistr(df$test_mean, "lognormal")$estimate  # lognormal
curve(dlnorm(x, meanlog=f[1], sdlog=f[2]), col="purple", lty=1, lwd=3, add=TRUE) # add lognormal curve
ks.test(df$test_mean, "plnorm", meanlog=f[1], sdlog=f[2]) # p < 2.2e-16
f = fitdistr(round(df$test_mean,0), "Poisson")$estimate  # Poisson
curve(dpois(round(x,0), lambda=f[1]), col="gold", lty=1, lwd=3, add=TRUE) # add Poisson curve
ks.test(df$test_mean, "ppois", lambda=f[1]) # p < 2.2e-16
f = fitdistr(round(df$test_mean,0), "negative binomial", lower=1e-6)$estimate  # negative binomial
curve(dnbinom(round(x,0), size=f[1], mu=f[2]), col="darkgray", lty=1, lwd=3, add=TRUE) # add negative binomial curve
ks.test(df$test_mean, "pnbinom", size=f[1], mu=f[2]) # p < 2.2e-16
f = fitdistr(df$test_mean, "exponential")$estimate  # exponential
curve(dexp(x, rate=f[1]), col="darkred", lty=1, lwd=3, add=TRUE) # add exponential curve
ks.test(df$test_mean, "pexp", rate=f[1]) # p = 0.0004948
df$test_mean_1 = df$test_mean + 1  ## add new column
hist(df$test_mean_1, main="Histogram of Recognition Error Rate", freq=FALSE, xlab="Error Rate", xlim=c(1,2)) # density (area sums to 1.00)
f = fitdistr(df$test_mean_1, "gamma")$estimate  # Gamma
curve(dgamma(x, shape=f[1], rate=f[2]), col="darkgreen", lty=1, lwd=3, add=TRUE) # add Gamma curve
ks.test(df$test_mean_1, "pgamma", shape=f[1], rate=f[2]) # p = 0.2983
## ANOVA assumptions
m = aov_ez(dv="test_mean", within=c("algoType","nTemplates"), between="ptype", id="pID", type=3, data=df)
r = residuals(m$lm)
length(r) # 162
mean(r); sum(r) # should be ~0
plot(r[1:length(r)]); abline(h=0) # should be random
hist(r) # should be normal
hist(r, freq=FALSE)
f = fitdistr(r, "normal")$estimate  # normal
curve(dnorm(x, mean=f[1], sd=f[2]), col="blue", lty=1, lwd=3, add=TRUE) # add normal curve
ks.test(r, "pnorm", mean=f[1], sd=f[2]) # p = 0.3995
ks.test(r, "pnorm", mean=f[1], sd=f[2]) # p-value = 0.6804 # p = 0.3995
rg = as.numeric(unlist(r + abs(min(r)) + 1))
hist(rg, freq=FALSE)
f = fitdistr(rg, "gamma")$estimate  # Gamma
curve(dgamma(x, shape=f[1], rate=f[2]), col="darkgreen", lty=1, lwd=3, add=TRUE) # add Gamma curve
ks.test(rg, "pgamma", shape=f[1], rate=f[2]) # p = 0.5885
ks.test(rg, "pgamma", shape=f[1], rate=f[2]) # p-value = 0.9216 # p = 0.5885
qqnorm(r); qqline(r) # Q-Q plot
shapiro.test(r)  # p = 8.676e-05
shapiro.test(r)  # p-value = 0.009343 # p = 8.676e-05
ad.test(r)  # p = 0.0105
ad.test(r) # p-value = 0.2144 # p = 0.0105
print(check_homogeneity(m)) # Levene's Test: p = 0.031
print(check_normality(m))   # p < .001
## LMM
m0 = lmer(test_mean ~ algoType * nTemplates * ptype + (1|pID), data=df)
summary(m0)
Anova(m0, type=3) ###
## Gamma
df$test_mean_1 = df$test_mean + 1  ## add new column
m1 = glmer(test_mean_1 ~ algoType * nTemplates * ptype + (1|pID), data=df, family=Gamma(link="inverse"))
summary(m1)
Anova(m1, type=3)
with(df, interaction.plot(
algoType, ptype, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Disability",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, Disability",
lwd=3,
lty=1,
col=c("darkblue","darkgreen")
))
with(df, interaction.plot(
algoType, nTemplates, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Templates",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, No. Templates",
lwd=3,
lty=1,
col=c("darkblue","darkgreen","goldenrod")
))
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m1, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
## Gamma
df$test_mean_1 = df$test_mean + 1  ## add new column
m1 = glmer(test_mean_1 ~ algoType * nTemplates * ptype + (1|pID), data=df, family=Gamma(link="inverse"))
summary(m1)
Anova(m1, type=3)
with(df, interaction.plot(
algoType, ptype, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Disability",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, Disability",
lwd=3,
lty=1,
col=c("darkblue","darkgreen")
))
# use Holm's sequential Bonferroni procedure to correct for multiple comparisons
summary(glht(m1, emm(pairwise ~ algoType)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ algoType*ptype)), test=adjusted(type="holm"))
# Post hoc pairwise comparisons, corrected with Holm's sequential Bonferroni procedure,
with(df, interaction.plot(
algoType, nTemplates, test_mean,
xlab="Algorithm",
ylab="Recognition Error Rate",
trace.label="Templates",
ylim=c(0,1),
main="Recognition Error Rate by Algorithm, No. Templates",
lwd=3,
lty=1,
col=c("darkblue","darkgreen","goldenrod")
))
summary(glht(m1, emm(pairwise ~ nTemplates)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ nTemplates)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ ptype)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ algoType*ptype)), test=adjusted(type="holm"))
summary(glht(m1, emm(pairwise ~ algoType*ptype)), test=adjusted(type="holm"))
